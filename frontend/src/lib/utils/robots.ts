export interface RobotsConfig {
  baseUrl: string;
  sitemapUrl: string;
  allowAll?: boolean;
  disallowPaths?: string[];
  allowPaths?: string[];
  crawlDelay?: number;
  userAgents?: {
    [key: string]: {
      allow?: string[];
      disallow?: string[];
    };
  };
}

export const generateRobotsTxt = (config: RobotsConfig): string => {
  const {
    baseUrl,
    sitemapUrl,
    allowAll = true,
    disallowPaths = [],
    allowPaths = [],
    crawlDelay = 1,
    userAgents = {}
  } = config;

  let robotsTxt = `# robots.txt for ${baseUrl}
# Generated by Next.js App Router
# Last updated: ${new Date().toISOString().split('T')[0]}
# 
# This file tells search engine crawlers which pages they can and cannot access.
# For more information, visit: https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt

`;

  // Default user agent rules
  if (allowAll) {
    robotsTxt += `# Allow all crawlers by default
User-agent: *
Allow: /

`;
  }

  // Add specific disallow paths
  if (disallowPaths.length > 0) {
    robotsTxt += `# Disallow access to private and system paths
`;
    disallowPaths.forEach(path => {
      robotsTxt += `Disallow: ${path}\n`;
    });
    robotsTxt += '\n';
  }

  // Add specific allow paths (usually not needed when allowAll is true)
  if (allowPaths.length > 0 && !allowAll) {
    robotsTxt += `# Explicitly allow specific paths
`;
    allowPaths.forEach(path => {
      robotsTxt += `Allow: ${path}\n`;
    });
    robotsTxt += '\n';
  }

  // Add crawl delay
  if (crawlDelay > 0) {
    robotsTxt += `# Crawl-delay for respectful crawling (seconds between requests)
Crawl-delay: ${crawlDelay}

`;
  }

  // Add specific user agent rules
  if (Object.keys(userAgents).length > 0) {
    robotsTxt += `# Specific rules for different user agents
`;
    Object.entries(userAgents).forEach(([agent, rules]) => {
      robotsTxt += `User-agent: ${agent}\n`;
      
      if (rules.allow) {
        rules.allow.forEach(path => {
          robotsTxt += `Allow: ${path}\n`;
        });
      }
      
      if (rules.disallow) {
        rules.disallow.forEach(path => {
          robotsTxt += `Disallow: ${path}\n`;
        });
      }
      
      robotsTxt += '\n';
    });
  }

  // Add sitemap
  robotsTxt += `# Sitemap location
Sitemap: ${sitemapUrl}

`;

  // Add host
  robotsTxt += `# Host
Host: ${baseUrl}`;

  return robotsTxt;
};

export const getDefaultRobotsConfig = (): RobotsConfig => {
  return {
    baseUrl: 'https://codexaclasses.com',
    sitemapUrl: 'https://codexaclasses.com/sitemap.xml',
    allowAll: true,
    disallowPaths: [
      '/admin/',
      '/dashboard/',
      '/api/',
      '/_next/',
      '/_vercel/',
      '/static/',
      '/private/',
      '/internal/',
      '/*.json$',
      '/*?*',
      '/enquiry',
      '/test-icons/',
      '/favicon.ico',
      '/robots.txt',
      '/sitemap.xml'
    ],
    allowPaths: [
      '/',
      '/courses',
      '/about',
      '/contact',
      '/super10',
      '/certificate',
      '/testimonials',
      '/login'
    ],
    crawlDelay: 1,
    userAgents: {
      'GPTBot': {
        disallow: ['/']
      },
      'ChatGPT-User': {
        disallow: ['/']
      },
      'CCBot': {
        disallow: ['/']
      },
      'anthropic-ai': {
        disallow: ['/']
      },
      'Omgilibot': {
        disallow: ['/']
      },
      'Googlebot': {
        allow: ['/'],
        disallow: ['/admin/', '/dashboard/', '/api/', '/_next/', '/_vercel/', '/static/', '/private/', '/internal/']
      },
      'Bingbot': {
        allow: ['/'],
        disallow: ['/admin/', '/dashboard/', '/api/', '/_next/', '/_vercel/', '/static/', '/private/', '/internal/']
      },
      'Slurp': {
        allow: ['/'],
        disallow: ['/admin/', '/dashboard/', '/api/', '/_next/', '/_vercel/', '/static/', '/private/', '/internal/']
      }
    }
  };
};
