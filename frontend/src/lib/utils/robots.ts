export interface RobotsConfig {
  baseUrl: string;
  sitemapUrl: string;
  allowAll?: boolean;
  disallowPaths?: string[];
  allowPaths?: string[];
  crawlDelay?: number;
  userAgents?: {
    [key: string]: {
      allow?: string[];
      disallow?: string[];
    };
  };
}

export const generateRobotsTxt = (config: RobotsConfig): string => {
  const {
    baseUrl,
    sitemapUrl,
    allowAll = true,
    disallowPaths = [],
    allowPaths = [],
    crawlDelay = 1,
    userAgents = {}
  } = config;

  let robotsTxt = `# robots.txt for ${baseUrl}
# Generated by Next.js App Router

`;

  // Default user agent rules
  if (allowAll) {
    robotsTxt += `# Allow all crawlers
User-agent: *
Allow: /

`;
  }

  // Add specific allow/disallow paths
  if (disallowPaths.length > 0) {
    disallowPaths.forEach(path => {
      robotsTxt += `Disallow: ${path}\n`;
    });
    robotsTxt += '\n';
  }

  if (allowPaths.length > 0) {
    allowPaths.forEach(path => {
      robotsTxt += `Allow: ${path}\n`;
    });
    robotsTxt += '\n';
  }

  // Add crawl delay
  if (crawlDelay > 0) {
    robotsTxt += `# Crawl-delay for respectful crawling
Crawl-delay: ${crawlDelay}

`;
  }

  // Add specific user agent rules
  Object.entries(userAgents).forEach(([agent, rules]) => {
    robotsTxt += `User-agent: ${agent}\n`;
    
    if (rules.allow) {
      rules.allow.forEach(path => {
        robotsTxt += `Allow: ${path}\n`;
      });
    }
    
    if (rules.disallow) {
      rules.disallow.forEach(path => {
        robotsTxt += `Disallow: ${path}\n`;
      });
    }
    
    robotsTxt += '\n';
  });

  // Add sitemap
  robotsTxt += `# Sitemap location
Sitemap: ${sitemapUrl}

`;

  // Add host
  robotsTxt += `# Host
Host: ${baseUrl}`;

  return robotsTxt;
};

export const getDefaultRobotsConfig = (): RobotsConfig => {
  return {
    baseUrl: 'https://codexaclasses.com',
    sitemapUrl: 'https://codexaclasses.com/sitemap.xml',
    allowAll: true,
    disallowPaths: [
      '/admin/',
      '/dashboard/',
      '/api/',
      '/_next/',
      '/_vercel/',
      '/static/',
      '/*.json$',
      '/*?*'
    ],
    allowPaths: [
      '/',
      '/courses',
      '/about',
      '/contact',
      '/super10',
      '/certificate',
      '/login'
    ],
    crawlDelay: 1,
    userAgents: {
      'GPTBot': {
        disallow: ['/']
      },
      'ChatGPT-User': {
        disallow: ['/']
      },
      'CCBot': {
        disallow: ['/']
      },
      'anthropic-ai': {
        disallow: ['/']
      },
      'Googlebot': {
        allow: ['/']
      },
      'Bingbot': {
        allow: ['/']
      }
    }
  };
};
